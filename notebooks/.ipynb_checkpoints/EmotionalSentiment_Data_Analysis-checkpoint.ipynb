{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will explore data gathered from the paper titled:\n",
    "\"GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception\"\n",
    "Weblink: https://arxiv.org/pdf/1912.03184.pdf\n",
    "\n",
    "Summary of Data: A collection of news headlines and the emotional respsone they generated\n",
    "\n",
    "Columns in the data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset with pandas\n",
    "\n",
    "# Changing the working directory to the raw data directory\n",
    "os.chdir('/home/george/Documents/Insight_DS_TO20A/Projects/EmotionalDetection/data/raw/GoodNewsEveryone')\n",
    "\n",
    "GoodNews_df_orig = pd.read_json('release_review.jsonl', lines=True)\n",
    "column_names = ['headline','phase1_rank','country','source','dominant_emotion','other_emotion','reader_emotion','intensity','readerperception']\n",
    "column_names_meta = ['phase1_rank','country']\n",
    "column_names_annotations = ['dominant_emotion','other_emotion','reader_emotion','intensity','readerperception']\n",
    "\n",
    "row_values = np.arange(5000) \n",
    "\n",
    "data_temp = pd.DataFrame(index=row_values, columns=column_names)\n",
    "\n",
    "for i in range(5000):\n",
    "#for i in range(10):    \n",
    "    data_temp.headline.iloc[i]         = GoodNews_df_orig.headline[i]\n",
    "    data_temp.phase1_rank.iloc[i]      = GoodNews_df_orig.meta.iloc[i]['phase1_rank']\n",
    "    data_temp.country.iloc[i]          = GoodNews_df_orig.meta.iloc[i]['country']\n",
    "    data_temp.source.iloc[i]           = GoodNews_df_orig.meta.iloc[i]['source']\n",
    "    data_temp.dominant_emotion.iloc[i] = GoodNews_df_orig.annotations.iloc[i]['dominant_emotion']['gold']\n",
    "    data_temp.other_emotion.iloc[i]    = GoodNews_df_orig.annotations.iloc[i]['other_emotions']['gold']  \n",
    "    data_temp.reader_emotion.iloc[i]   = GoodNews_df_orig.annotations.iloc[i]['reader_emotions']['gold'] \n",
    "    data_temp.intensity.iloc[i]        = GoodNews_df_orig.annotations.iloc[i]['intensity']['gold']\n",
    "    data_temp.readerperception.iloc[i] = GoodNews_df_orig.annotations.iloc[i]['phase1_readerperception']['gold'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number (out of 5000) of negative emotional headlines = 3582\n",
      "The number (out of 5000) of positive emotional headlines = 1418\n"
     ]
    }
   ],
   "source": [
    "# Minimalist appraoch to labelling the above data set: \n",
    "# Create a binary classification scheme = the headline has a \"+\" or \"-\" emotional response.\n",
    "## Only considering the dominant emotional responses\n",
    "\n",
    "# Printing all emotional tags for the dominant emotion\n",
    "# emotional_responses = data_temp.dominant_emotion.unique()\n",
    "# array(['anger', 'negative_surprise', 'sadness', 'disgust',\n",
    "#        'positive_surprise', 'positive_anticipation_including_optimism',\n",
    "#        'guilt', 'annoyance', 'negative_anticipation_including_pessimism',\n",
    "#        'pride', 'fear', 'shame', 'trust', 'love_including_like', 'joy'],\n",
    "#       dtype=object)\n",
    "\n",
    "pos_emo_response = np.array(['positive_surprise', 'positive_anticipation_including_optimism','pride','trust', 'love_including_like', 'joy'])\n",
    "\n",
    "# Creating a column to store the \"binary\" class for training the MVP\n",
    "## NOTE: We only need to chnage the values for the positive emotional responses\n",
    "data_temp['BinaryEmoLabel'] = 0\n",
    "for i in range(data_temp.shape[0]):\n",
    "    emo_temp = data_temp.dominant_emotion.iloc[i]\n",
    "    for j in range(pos_emo_response.shape[0]):\n",
    "        if emo_temp == pos_emo_response[j]:\n",
    "            data_temp.BinaryEmoLabel.iloc[i] = 1\n",
    "\n",
    "            \n",
    "# Printing the number of each class:\n",
    "print(\"The number (out of 5000) of negative emotional headlines =\", data_temp.BinaryEmoLabel.shape[0] - data_temp.BinaryEmoLabel.sum())\n",
    "print(\"The number (out of 5000) of positive emotional headlines =\", data_temp.BinaryEmoLabel.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the text / headlines\n",
    "#### Initially the steps which follow were inspired by the Medium Blog Posts:\n",
    "#### https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\n",
    "#### https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a\n",
    "#### Written By: Aaron Kub\n",
    "\n",
    "#### https://towardsdatascience.com/twitter-sentiment-analysis-classification-using-nltk-python-fa912578614c\n",
    "#### Written By: Mohamed Afham ** Follwoing this scheme to begin the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f459447fdd0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The Data Exploration Portion\n",
    "\n",
    "# The number count of each label\n",
    "sns.countplot(x= 'BinaryEmoLabel',data = data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "## Punctuation Removal\n",
    "def form_sentence(headline):\n",
    "    headline_blob = TextBlob(headline)\n",
    "    return ' '.join(headline_blob.words)\n",
    "\n",
    "data_temp['HL_PuncRem_1'] = 'NaN'\n",
    "for i in range(data_temp.shape[0]):\n",
    "    data_temp.HL_PuncRem_1.iloc[i] = form_sentence(data_temp['headline'].iloc[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Stop-Words (e.g: is, are, have)\n",
    "def no_user_alpha(headline):\n",
    "    headline_list = [ele for ele in headline.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in headline_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean_mess\n",
    "\n",
    "data_temp['HL_StopWords_2'] = 'NaN'\n",
    "for i in range(data_temp.shape[0]):\n",
    "    data_temp.HL_StopWords_2.iloc[i] = no_user_alpha(data_temp['HL_PuncRem_1'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cops', 'One', 'Village', 'Convicted', 'Crimes', 'Say']\n",
      "Cops in One Village Have Been Convicted of 70 Crimes. Here’s What They Had to Say About It.\n"
     ]
    }
   ],
   "source": [
    "print(data_temp.HL_StopWords_2.iloc[0])\n",
    "print(data_temp.headline.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Text -- NLTK’s built-in WordNetLemmatizer does this \n",
    "def normalization(headline_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_headline = []\n",
    "        for word in headline_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_headline.append(normalized_text)\n",
    "        return normalized_headline\n",
    "    \n",
    "data_temp['HL_Normalize_3'] = 'NaN'\n",
    "for i in range(data_temp.shape[0]):\n",
    "    data_temp.HL_Normalize_3.iloc[i] = normalization(data_temp['HL_StopWords_2'].iloc[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cops', 'One', 'Village', 'Convicted', 'Crimes', 'Say']\n",
      "Cops in One Village Have Been Convicted of 70 Crimes. Here’s What They Had to Say About It.\n"
     ]
    }
   ],
   "source": [
    "print(data_temp.HL_Normalize_3.iloc[0])\n",
    "print(data_temp.headline.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all the above text pre-processing into one function \n",
    "\n",
    "def text_processing(headline):\n",
    "    \n",
    "    #Generating the list of words in the headline (hastags and other punctuations removed)\n",
    "    def form_sentence(headline):\n",
    "        headline_blob = TextBlob(headline)\n",
    "        return ' '.join(headline_blob.words)\n",
    "    new_headline = form_sentence(headline)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(headline):\n",
    "        headline_list = [ele for ele in headline.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in headline_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_headline = no_user_alpha(new_headline)\n",
    "    \n",
    "    #Normalizing the words in headlines \n",
    "    def normalization(headline_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_headline = []\n",
    "        for word in headline_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_headline.append(normalized_text)\n",
    "        return normalized_headline\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn: Pre-defining a workflow of algorithm (Niave-Bayse Classifier)\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the Data into a Training, Validation, and Test Set\n",
    "## Fractions are: Training = 80%, Validation = 10%, Test = 10 %\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "\n",
    "## For the time being, the following code forces the training, validation, and test data sets are balanced\n",
    "num_pos_labels = data_temp['BinaryEmoLabel'].sum() # 1418 headlines\n",
    "num_neg_labels = data_temp.shape[0] - num_pos_labels # 3582 headlines\n",
    "num_neg_labels_2drop = num_neg_labels - num_pos_labels # 2164 headlines\n",
    "cnt = 0\n",
    "data_temp_bal = data_temp.copy(deep=True)\n",
    "# remove the first \"num_neg_2drop\" \n",
    "        \n",
    "index_Label0 = data_temp_bal.index[data_temp_bal['BinaryEmoLabel'] == 0].tolist()\n",
    "\n",
    "\n",
    "Labels = data_temp.BinaryEmoLabel\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_temp, Labels, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Number of Headlines in the balanced Data Set are: 2836\n"
     ]
    }
   ],
   "source": [
    "test = data_temp_bal.drop(data_temp_bal.index[index_Label0[0:num_neg_labels_2drop]])\n",
    "\n",
    "print(\"The Total Number of Headlines in the balanced Data Set are:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.65      0.67       294\n",
      "           1       0.65      0.68      0.66       274\n",
      "\n",
      "    accuracy                           0.67       568\n",
      "   macro avg       0.67      0.67      0.67       568\n",
      "weighted avg       0.67      0.67      0.67       568\n",
      "\n",
      "[[192 102]\n",
      " [ 87 187]]\n",
      "0.6672535211267606\n",
      "The accuracy of labelling all headlines NEGATIVE is: 0.6019040902679831\n",
      "This model's accuracy is better than the Niave assumption by: 0.06534943085877754\n"
     ]
    }
   ],
   "source": [
    "### Trainiing Model ####\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(test['headline'], test['BinaryEmoLabel'], test_size=0.2)\n",
    "pipeline.fit(msg_train,label_train)\n",
    "predictions = pipeline.predict(msg_test)\n",
    "print(classification_report(predictions,label_test))\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))\n",
    "# What would the accuracy of the Training Data Set is we niavely set all labels to 0\n",
    "print(\"The accuracy of labelling all headlines NEGATIVE is:\",1-label_train.sum()/len(test))\n",
    "\n",
    "print(\"This model's accuracy is better than the Niave assumption by:\", accuracy_score(predictions,label_test) - 1+label_train.sum()/len(test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of labelling all headlines NEGATIVE is: 0.5987306064880114\n"
     ]
    }
   ],
   "source": [
    "# What would the accuracy of the Training Data Set is we niavely set all labels to 0\n",
    "print(\"The accuracy of labelling all headlines NEGATIVE is:\",1-label_train.sum()/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

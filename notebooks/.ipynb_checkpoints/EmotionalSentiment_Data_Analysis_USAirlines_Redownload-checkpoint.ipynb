{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will explore data gathered from the US Airlines Sentiment Data Set\n",
    "\n",
    "Summary of Data: Twitter responses towards US Airlines and the emotional state of each party.\n",
    "\n",
    "Data Information:\n",
    "Length of data set: 14,640\n",
    "Columns in the data set:'tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
    "       'negativereason', 'negativereason_confidence', 'airline',\n",
    "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
    "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
    "       'tweet_location', 'user_timezone'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Python modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the Jupyter Notebooks fills the web browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset with pandas\n",
    "\n",
    "# Changing the working directory to the raw data directory\n",
    "os.chdir('/home/george/Documents/Insight_DS_TO20A/Projects/EmotionalDetection/data/raw/US_Airline_Sentiment')\n",
    "data_temp = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "        'negativereason', 'negativereason_confidence', 'airline',\n",
       "        'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "        'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "        'tweet_location', 'user_timezone'],\n",
       "       dtype='object'), (14640, 15))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_temp.columns, data_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tweets are: 10522\n",
      "The number (out of 5000) of negative emotional headlines = 7397\n",
      "The number (out of 5000) of positive emotional headlines = 3125\n",
      "The niave classification (1 class fit to all) is = 0.7030032313248431\n"
     ]
    }
   ],
   "source": [
    "# Minimalist appraoch to labelling the above data set: \n",
    "# Create a binary classification scheme = the headline has a \"+\" or \"-\" emotional response.\n",
    "## Only collect the responses that gave a positive or negative emotional response from the airline\n",
    "\n",
    "# data_pn = data_temp[data_temp.airline_sentiment != 'neutral'] \n",
    "# data_pn.airline_sentiment.unique(), data_pn.shape  \n",
    "\n",
    "# Droping labels with nan and converting them\n",
    "#data_temp.negativereason_confidence.dropna(how='all',inplace=True)\n",
    "#data_temp['negativereason_confidence'] = data_temp.negativereason_confidence.round(0).astype(int)\n",
    "#data_temp.negativereason_confidence.dropna(how='all',inplace=True)\n",
    "#data_temp.negativereason_confidence.unique()\n",
    "\n",
    "\n",
    "# # Gathering the text and labels\n",
    "\n",
    "text = data_temp.text.copy(deep=True)\n",
    "labels_confvalues = data_temp.negativereason_confidence\n",
    "labels_confvalues.unique()\n",
    "# # Pre-processing labels. Removing \"inf\" and nan\n",
    "labels_confvalues.replace([np.inf, -np.inf], np.nan)\n",
    "labels_confvalues.dropna()\n",
    "\n",
    "# # Seperating the labels by seperating the confidence labels to be:\n",
    "# # negative > 0.5, positive is <= 0.5\n",
    "labels_confvalues.dropna(how='all', inplace=True)\n",
    "labels = 1-labels_confvalues.round(0).astype(int)\n",
    "filter_text = labels.index\n",
    "text = text[filter_text]\n",
    "\n",
    "# Printing the number of each class:\n",
    "print(\"The total number of tweets are:\", labels.shape[0])\n",
    "print(\"The number (out of 5000) of negative emotional headlines =\", labels.shape[0] - labels.sum()) # labeled with 0s\n",
    "print(\"The number (out of 5000) of positive emotional headlines =\", labels.sum()) # Labelled with 1s\n",
    "print(\"The niave classification (1 class fit to all) is =\", 1-labels.sum()/len(labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7397\n",
      "3125\n",
      "4272\n",
      "The total length of the label series =  (6250,)\n",
      "The total number of positive labels = 3125\n"
     ]
    }
   ],
   "source": [
    "# Forcing a balance dataset -- NIAVELY REMOVING POSITIVE TWEETS To BALANCE THE DATASET\n",
    "\n",
    "## Splitting the Data into a Training, Validation, and Test Set\n",
    "## Fractions are: Training = 80%, Validation = 10%, Test = 10 %\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "\n",
    "## For the time being, the following code forces the training, validation, and test data sets are balanced\n",
    "num_pos_labels = labels.sum() # 1418 headlines\n",
    "num_neg_labels = labels.shape[0] - num_pos_labels # 3582 headlines\n",
    "num_pos_labels_2drop = num_pos_labels - num_neg_labels # 2164 headlines\n",
    "print(num_pos_labels)\n",
    "print(num_neg_labels)\n",
    "print(num_pos_labels_2drop)\n",
    "# cnt = 0\n",
    "\n",
    "index_pos = labels[labels == 1].index\n",
    "index_neg = labels[labels == 0].index\n",
    "labels_temp_bal_pos = labels[index_pos[num_pos_labels_2drop:]]\n",
    "labels_temp_bal_neg = labels[index_neg]\n",
    "\n",
    "labels_temp_bal = labels_temp_bal_neg.combine(labels_temp_bal_pos, max, fill_value=0)\n",
    "\n",
    "print(\"The total length of the label series = \", labels_temp_bal.shape)\n",
    "print(\"The total number of positive labels =\",labels_temp_bal.sum())\n",
    "labels_temp_bal.loc[14638], labels.loc[14638]\n",
    "\n",
    "## Ensuring we use the appropriate text from the balanced labeled series\n",
    "labels_temp_bal_indices = labels_temp_bal.index\n",
    "text_bal = text[labels_temp_bal_indices]\n",
    "\n",
    "# cnt = 0\n",
    "# for i in range(labels_temp_bal.shape[0]):\n",
    "#     if labels_temp_bal.iloc[i] == 1 and cnt < (num_pos_labels_2drop+1):\n",
    "#         labels_temp_bal[i].drop(inplace=True)\n",
    "#         cnt += 1\n",
    "\n",
    "# index_Label0 = data_temp_bal.index[data_temp_bal['BinaryEmoLabel'] == 0].tolist()\n",
    "\n",
    "\n",
    "# Labels = data_temp.BinaryEmoLabel\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_temp, Labels, test_size=0.2, random_state=1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the text / headlines\n",
    "#### Initially the steps which follow were inspired by the Medium Blog Posts:\n",
    "#### https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184\n",
    "#### https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a\n",
    "#### Written By: Aaron Kub\n",
    "\n",
    "#### https://towardsdatascience.com/twitter-sentiment-analysis-classification-using-nltk-python-fa912578614c\n",
    "#### Written By: Mohamed Afham ** Follwoing this scheme to begin the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all the above text pre-processing into one function \n",
    "\n",
    "def text_processing(headline):\n",
    "    \n",
    "    #Generating the list of words in the headline (hastags and other punctuations removed)\n",
    "    def form_sentence(headline):\n",
    "        headline_blob = TextBlob(headline)\n",
    "        return ' '.join(headline_blob.words)\n",
    "    new_headline = form_sentence(headline)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(headline):\n",
    "        headline_list = [ele for ele in headline.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in headline_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_headline = no_user_alpha(new_headline)\n",
    "    \n",
    "    #Normalizing the words in headlines \n",
    "    def normalization(headline_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_headline = []\n",
    "        for word in headline_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_text = lem.lemmatize(normalized_text,'a')\n",
    "            normalized_text = lem.lemmatize(normalized_text,'n')\n",
    "            normalized_headline.append(normalized_text)\n",
    "        return normalized_headline\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Int2EmotionConverter(IntEmo):\n",
    "    \"\"\"\n",
    "    Converts the integer value of the emotion predicted to the emotions word in English\n",
    "    \"\"\"\n",
    "    EmoWords = pd.DataFrame(['Happy','Sad'])\n",
    "    return EmoWords.iloc[IntEmo][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn: Pre-defining a workflow of algorithm (Niave-Bayse Classifier)\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),                      # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_wSMOTE = make_pipeline( CountVectorizer(analyzer=text_processing),\n",
    "                          TfidfTransformer(),\n",
    "                          SMOTE(random_state=4) ,\n",
    "                          MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77       277\n",
      "           1       0.84      0.75      0.79       348\n",
      "\n",
      "    accuracy                           0.78       625\n",
      "   macro avg       0.78      0.79      0.78       625\n",
      "weighted avg       0.79      0.78      0.78       625\n",
      "\n",
      "[[229  48]\n",
      " [ 88 260]]\n",
      "0.7824\n",
      "The accuracy of labelling all headlines NEGATIVE is: 0.5072\n",
      "This model's accuracy is better than the Niave assumption by: 0.2752\n"
     ]
    }
   ],
   "source": [
    "##### Training Model #### -- FOR THE MANUALLY BALANCED DATA SET\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(text_bal, labels_temp_bal, test_size=0.1, random_state=4)#, stratify=labels_temp_bal)\n",
    "pipeline.fit(msg_train,label_train)\n",
    "predictions = pipeline.predict(msg_test)\n",
    "print(classification_report(predictions,label_test))\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))\n",
    "# What would the accuracy of the Training Data Set is we niavely set all labels to 0\n",
    "print(\"The accuracy of labelling all headlines NEGATIVE is:\",1-label_test.sum()/len(label_test))\n",
    "\n",
    "print(\"This model's accuracy is better than the Niave assumption by:\", accuracy_score(predictions,label_test) - 1+label_test.sum()/len(label_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Set = 9469\n",
      "Fraction of Neg Train Labels =  0.7047206674411237\n",
      "Expected Number of Total Training Tweets after SMOTE is applied = 13346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.47      0.61       601\n",
      "           1       0.56      0.90      0.69       452\n",
      "\n",
      "    accuracy                           0.66      1053\n",
      "   macro avg       0.71      0.69      0.65      1053\n",
      "weighted avg       0.74      0.66      0.65      1053\n",
      "\n",
      "[[285 316]\n",
      " [ 44 408]]\n",
      "0.6581196581196581\n",
      "The accuracy of labelling all headlines NEGATIVE is: 0.6875593542260209\n",
      "This model's accuracy is better than the Niave assumption by: 0.345679012345679\n"
     ]
    }
   ],
   "source": [
    "##### Training Model #### -- FOR THE SMOTE PIPLINE -- ALL DATA!\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(text, labels, test_size=0.1, random_state=4)# stratify=labels)\n",
    "print('Size of Training Set =',msg_train.shape[0])\n",
    "print('Fraction of Neg Train Labels = ', label_train.sum()*1.0 / len(label_train) )\n",
    "print('Expected Number of Total Training Tweets after SMOTE is applied =', label_train.sum()*2)\n",
    "pipeline_wSMOTE.fit(msg_train,label_train)\n",
    "predictions = pipeline.predict(msg_test)\n",
    "print(classification_report(predictions,label_test))\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))\n",
    "# What would the accuracy of the Training Data Set is we niavely set all labels to 0\n",
    "print(\"The accuracy of labelling all headlines NEGATIVE is:\", max(1-label_test.sum()/len(label_test),label_test.sum()/len(label_test) ))\n",
    "\n",
    "print(\"This model's accuracy is better than the Niave assumption by:\", accuracy_score(predictions,label_test) - 1+label_test.sum()/len(label_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6673"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.sum()*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(labels_temp_bal)/len(labels_temp_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "import joblib\n",
    "\n",
    "filename = 'NBC_USAirlines_model_Acc82p40.sav'\n",
    "joblib.dump(pipeline, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading the model trained\n",
    "filename = 'NBC_USAirlines_model_Acc83p36.sav'\n",
    "loaded_pipeline = joblib.load(filename)\n",
    "\n",
    "loaded_predictions = loaded_pipeline.predict(msg_test)\n",
    "print(classification_report(loaded_predictions,label_test))\n",
    "print(confusion_matrix(loaded_predictions,label_test))\n",
    "print(accuracy_score(loaded_predictions,label_test))\n",
    "# What would the accuracy of the Training Data Set is we niavely set all labels to 0\n",
    "print(\"The accuracy of labelling all headlines NEGATIVE is:\",1-label_train.sum()/len(label_train))\n",
    "\n",
    "print(\"This model's accuracy is better than the Niave assumption by:\", accuracy_score(loaded_predictions,label_test) - 1+label_train.sum()/len(label_train)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_index = 90\n",
    "\n",
    "temp_text = pd.Series([])\n",
    "temp_text[0] = text.iloc[dummy_index]\n",
    "type(temp_text)\n",
    "\n",
    "out_temp = pipeline.predict(temp_text)\n",
    "print(\"The sentence input was:\\n\",temp_text[0])\n",
    "print(\"The emotional prediction was:\", Int2EmotionConverter(out_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly print a positive review and a negative review -- MANUAL INSPECTION OF RESULTS\n",
    "\n",
    "# Collecting all preditions\n",
    "all_preds = pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all indices for \"positive\" and \"negative\" reviews\n",
    "pred_neg_indices = np.where(all_preds == 0)[0] # all_preds[all_preds == 0].index \n",
    "pred_pos_indices = np.where(all_preds == 1)[0] # all_preds[all_preds == 1].index\n",
    "\n",
    "neg_index_ran = np.random.choice(pred_neg_indices,1)[0]\n",
    "pos_index_ran = np.random.choice(pred_pos_indices,1)[0]\n",
    "#print(neg_index_ran)\n",
    "#print(pos_index_ran)\n",
    "print(\"THIS IS THE SAD CASE!!\")\n",
    "# Printing the results of the negative text\n",
    "print(\"The sentence input was:\\n\",text.iloc[neg_index_ran])\n",
    "print(\"The emotional prediction was:\", Int2EmotionConverter(all_preds[neg_index_ran]))\n",
    "print(\"________________________\")\n",
    "print(\"THIS IS THE HAPPY CASE!!\")\n",
    "# Printing the results of the positive text\n",
    "print(\"The sentence input was:\\n\",text.iloc[pos_index_ran])\n",
    "print(\"The emotional prediction was:\", Int2EmotionConverter(all_preds[pos_index_ran]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now predicting the emotional sentiment (neg., pos.) given a user string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_text = pd.Series([])\n",
    "temp_text[0] = \"I'm Sad!\"\n",
    "\n",
    "out_temp = pipeline.predict(temp_text)\n",
    "print(\"The sentence input was:\\n\",temp_text[0])\n",
    "print(\"The emotional prediction was:\", Int2EmotionConverter(out_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text\n",
    "out_temp[0]\n",
    "Int2EmotionConverter(out_temp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL CONVERSATION EXAMPLE CASE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing the real conversation\n",
    "os.chdir('/home/george/Documents/Insight_DS_TO20A/Projects/EmotionalDetection/data/raw')\n",
    "real_conv = pd.read_csv('real_chat.txt',sep=\"\\n\", header=None, dtype=str) \n",
    "real_conv = real_conv[0] # converts the DataFrame to a DataSeries \n",
    "                         # as desired for the label predictions\n",
    "\n",
    "\n",
    "# Generating the emotional predictions from the above text\n",
    "emos_real_chat = pipeline.predict(real_conv)\n",
    "\n",
    "# generating the speaker/writer\n",
    "speaker = pd.Series([])\n",
    "for i in range(real_conv.shape[0]):\n",
    "    speaker[i] = real_conv.iloc[i].split()[0][:-1] \n",
    "\n",
    "speaker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(speaker), type(emos_real_chat)\n",
    "chat_df = pd.DataFrame([])\n",
    "chat_df['speaker'] = speaker\n",
    "chat_df['emotions'] = emos_real_chat\n",
    "chat_df.to_csv('chat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df = pd.read_csv('chat_df.csv')#, chat_df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.palplot(sns.color_palette(\"husl\", 8)) # \n",
    "index_agent = chat_df.index[chat_df['speaker'] == 'CS_Agent' ].tolist()\n",
    "index_customer = chat_df.index[chat_df['speaker'] == 'Customer' ].tolist()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(index_agent,chat_df['emotions'][index_agent],label='agent',lw=7,color='lightgreen')\n",
    "plt.plot(index_customer,chat_df['emotions'][index_customer],label='customer', lw=7,color='lightblue')\n",
    "\n",
    "#plt.axvline(x=37,linewidth=10,color='r',label=\"Intervention\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.,prop={'size': 30} )\n",
    "plt.xlabel('Tweet Number', size=40)\n",
    "plt.ylabel('Emotion',size=40)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks([0,1],labels=['Positive','Negative'],fontsize=40)\n",
    "\n",
    "#plt.text(10.1,0,'Intervention',rotation=90)\n",
    "# Changing the y-labels to \"Sad\" (0) and \"Happy\" (1)\n",
    "# labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "# labels[0] = 'Sad'\n",
    "# labels[1] = 'Happy'\n",
    "plt.xlim(0,10)\n",
    "plt.savefig(\"Preliminary_AgentCustomer_Emotional_Output.png\", bbox_inches='tight', dpi=100)\n",
    "plt.plot()\n",
    "#final_df.plot(x='index',y='emotions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer_Chat = chat_df[chat_df[\"speaker\"] == 'Customer']\n",
    "CSAgent_Chat  = chat_df[chat_df[\"speaker\"] == 'CS_Agent']\n",
    "len(Customer_Chat) , len(CSAgent_Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just animated 1 plot by examplining the two-graph animation code below\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "# ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "t = np.linspace(1, len(Customer_Chat), len(Customer_Chat))\n",
    "x = np.array(Customer_Chat[\"emotions\"])\n",
    "#y = CSAgent_Chat[\"emotions\"]\n",
    "\n",
    "ax1.set_ylabel('Customer')\n",
    "ax1.set_xlim(0, len(Customer_Chat))\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "plt.setp(ax1.get_xticklabels(),visible=True)\n",
    "plt.yticks([0,1],labels=['Positive','Negative'],fontsize=10)\n",
    "# ax2.set_xlabel('t')\n",
    "# ax2.set_ylabel(u'CS_Agent')\n",
    "# ax2.set_xlim(0, max(chat_df.index))\n",
    "# ax2.set_ylim(0, 1)\n",
    "\n",
    "lines = []\n",
    "for i in range(len(t)):    \n",
    "    head = i - 1\n",
    "    head_slice = (t > t[i] - 1.0) & (t < t[i])\n",
    "    line1,  = ax1.plot(t[:i], x[:i], color='black')\n",
    "    line1a, = ax1.plot(t[head_slice], x[head_slice], color='red', linewidth=2)\n",
    "    line1e, = ax1.plot(t[head], x[head], color='red', marker='o', markeredgecolor='r')\n",
    "    lines.append([line1,line1a,line1e])\n",
    "\n",
    "\n",
    "# Build the animation using ArtistAnimation function\n",
    "\n",
    "ani = animation.ArtistAnimation(fig,lines,interval=100,blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1, len(Customer_Chat), len(Customer_Chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now creating a new customer and agent conversation arrays which have the same (full conversation) index length\n",
    "\n",
    "CSChat_Emotions = pd.DataFrame([])\n",
    "CSChat_Emotions[\"Customer_Emotions\"] = np.zeros([chat_df.shape[0]])\n",
    "CSChat_Emotions[\"Agent_Emotions\"] = np.zeros([chat_df.shape[0]])\n",
    "Customer_Previous_Emotion = 0 # We presume both parties start in a positive state\n",
    "CS_Agent_Previous_Emotion = 0 # \"\"\n",
    "for i in range(chat_df.shape[0]):\n",
    "    current_speaker = chat_df['speaker'].iloc[i]\n",
    "    print(current_speaker)\n",
    "    if current_speaker == \"Customer\":\n",
    "        CSChat_Emotions[\"Customer_Emotions\"].iloc[i] = chat_df['emotions'].iloc[i]\n",
    "        CSChat_Emotions[\"Agent_Emotions\"].iloc[i] = CS_Agent_Previous_Emotion\n",
    "        Customer_Previous_Emotion = chat_df['emotions'].iloc[i]\n",
    "    #else:\n",
    "    elif current_speaker == \"CS_Agent\":\n",
    "        CSChat_Emotions[\"Customer_Emotions\"].iloc[i] = Customer_Previous_Emotion\n",
    "        CSChat_Emotions[\"Agent_Emotions\"].iloc[i] = chat_df['emotions'].iloc[i]\n",
    "        CS_Agent_Previous_Emotion = chat_df['emotions'].iloc[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, max(CSChat_Emotions.index), max(CSChat_Emotions.index)+1)\n",
    "t, t.shape, CSChat_Emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install imagemagick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plotting both the Customer and the Agent in a single animated plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "#sns.palplot(sns.color_palette(\"husl\", 8)) # \n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "t = np.linspace(1, len(CSChat_Emotions), len(CSChat_Emotions))\n",
    "x = CSChat_Emotions[\"Customer_Emotions\"]\n",
    "y = CSChat_Emotions[\"Agent_Emotions\"]\n",
    "# print(t.shape)\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "ax1.set_ylabel(u'Customer')\n",
    "ax1.yaxis.set_label_position(\"right\")\n",
    "ax1.set_xlim(0, max(CSChat_Emotions.index))\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "plt.setp(ax1.get_xticklabels(),visible=False)\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_yticklabels(labels=['Postive', 'Negative'])\n",
    "ax1.set_title('Real-time Emotions: Customer Service Text Conversation', size=15)\n",
    "\n",
    "ax2.set_xlabel('Text Message Number')\n",
    "ax2.set_ylabel(u'CS Agent')\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "ax2.set_xlim(0, max(CSChat_Emotions.index))\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "ax2.set_yticks([0, 1])\n",
    "ax2.set_yticklabels(labels=['Postive', 'Negative'])\n",
    "\n",
    "lines = []\n",
    "for i in range(1,len(t)): \n",
    "#     print(i)\n",
    "    head = i - 1\n",
    "    head_slice = (t > t[i] - 1.0) & (t < t[i])\n",
    "    line1,  = ax1.plot(t[:i], x[:i], color='lightblue')\n",
    "    line1a, = ax1.plot(t[head_slice], x[head_slice], color='red', linewidth=2)\n",
    "    line1e, = ax1.plot(t[head], x[head], color='red', marker='o', markeredgecolor='r')\n",
    "    line2,  = ax2.plot(t[:i], y[:i], color='lightgreen')\n",
    "    line2a, = ax2.plot(t[head_slice], y[head_slice], color='red', linewidth=2)\n",
    "    line2e, = ax2.plot(t[head], y[head], color='red', marker='o', markeredgecolor='r')\n",
    "    lines.append([line1,line1a,line1e,line2,line2a,line2e])\n",
    "\n",
    "\n",
    "# Build the animation using ArtistAnimation function\n",
    "\n",
    "ani = animation.ArtistAnimation(fig,lines,interval=125,blit=True)\n",
    "ani.save('animation.gif', writer='imagemagick', fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(CSChat_Emotions.index)\n",
    "# line1a, = ax1.plot(t[head_slice], x[head_slice], color='red', linewidth=2)\n",
    "# line1e, = ax1.plot(t[head], x[head], color='red', marker='o', markeredgecolor='r')\n",
    "# head, t\n",
    "t = np.linspace(1, len(CSChat_Emotions), len(CSChat_Emotions))\n",
    "print(t)\n",
    "for i in t:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Support on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the \"Customer Support on Twitter\"\n",
    "os.chdir('/home/george/Documents/Insight_DS_TO20A/Projects/EmotionalDetection/data/raw/CustomerSupportTwitter')\n",
    "cst_orig = pd.read_csv('twcs.csv')\n",
    "print(cst_orig.shape)\n",
    "cst_orig.dropna(inplace=True) # removing the NaN valued rows as we desire a conversation (3 tweets)\n",
    "                              # which requires all tweet_id place holders to be non-empty\n",
    "print(cst_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cst_orig.columns, cst_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now parsing the response_tweet_id and in_reponse_to_tweet_idt\n",
    "#print(type(cst_orig['response_tweet_id']))\n",
    "#print(np.fromstring(cst_orig['response_tweet_id'][5], dtype=int, sep=','))\n",
    "\n",
    "cst_orig_np = cst_orig['response_tweet_id'].to_numpy()\n",
    "\n",
    "# type(str.split(cst_orig_np[5],','))\n",
    "# int(str.split(cst_orig_np[5],',')[0])\n",
    "\n",
    "# list(map(int, cst_orig_np))\n",
    "print((cst_orig['response_tweet_id'].iloc[5]))\n",
    "# list(map(int,cst_orig_np[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## identifying conversations -- gathering the unqiue pairs of tweets\n",
    "tweetID_pairs = pd.DataFrame([])\n",
    "tweetID_pairs['author_id'] = cst_orig['author_id']\n",
    "tweetID_pairs['response_tweet_id'] = cst_orig['response_tweet_id']\n",
    "print(\"The length of original tweet id-pair dataframe is:\", tweetID_pairs.shape[0])\n",
    "\n",
    "# Generate a numpy array to sort the rows\n",
    "tweetID_pairs_np = tweetID_pairs.to_numpy(copy=True)\n",
    "tweetID_pairs_np = np.sort(tweetID_pairs_np)\n",
    "\n",
    "tweetID_pairsSrtd = pd.DataFrame([])\n",
    "tweetID_pairsSrtd['ID1'] = tweetID_pairs_np[:,0]\n",
    "tweetID_pairsSrtd['ID2'] = tweetID_pairs_np[:,1]\n",
    "tweetID_pairsSrtd.drop_duplicates(inplace=True)\n",
    "print(\"The length of the unique tweet pair\", tweetID_pairsSrtd.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetID_pairsSrtd['ID1'].unique().shape, tweetID_pairsSrtd['ID2'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connecting twiter conversations together through 'tweet_id', 'response_tweet_id', 'in_response_to_tweet_id'\n",
    "convos = pd.DataFrame([])\n",
    "\n",
    "for i in range(cst_orig.shape[0]):\n",
    "    convos[\"BaseID\"].iloc[i] = cst_orig['tweet_id']\n",
    "    response_temp = cst_orig['response_tweet_id']\n",
    "    resp_list = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting text to an image\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "lines = [\"In the old #BILGETANK we'll keep you in the know\",\n",
    "         \"In the old #BILGETANK we'll fix your techie woes\",\n",
    "         \"And we'll make things\",\n",
    "         \"And we'll break things\",\n",
    "         \"'til we're altogether aching\",\n",
    "         \"Then we'll grab a cup of grog down in the old #BILGETANK\"]\n",
    "\n",
    "i_bad = 3 # This will be output by the model -- Negative Index\n",
    "\n",
    "img = Image.new('RGB', (1024, 1024), color = (240, 255, 240))\n",
    "font = ImageFont.truetype('/usr/share/fonts/truetype/freefont/FreeSerif.ttf', 32) \n",
    "d = ImageDraw.Draw(img)\n",
    "for i in range(len(lines)):\n",
    "    message = lines[i]\n",
    "    if i != i_bad:\n",
    "        d.text((10,10+36*i), message, fill=(100,20,255), font=font)\n",
    "    else:\n",
    "        d.text((10,10+36*i),\"*\"+message+\"*\", fill=(255,20,20), font=font)\n",
    "file_name = \"Text_%i\" %(i)\n",
    "print(file_name)\n",
    "img.save('pil_text.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------The Test Dev. Case is Completed ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX: The Individual Functions and Examples cases to showcase how they work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Punctuation Removal\n",
    "def form_sentence(headline):\n",
    "    headline_blob = TextBlob(headline)\n",
    "    return ' '.join(headline_blob.words)\n",
    "\n",
    "data_temp['HL_PuncRem_1'] = 'NaN'\n",
    "for i in range(data_temp.shape[0]):\n",
    "    data_temp.HL_PuncRem_1.iloc[i] = form_sentence(data_temp['headline'].iloc[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing Stop-Words (e.g: is, are, have)\n",
    "def no_user_alpha(headline):\n",
    "    headline_list = [ele for ele in headline.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in headline_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean_mess\n",
    "\n",
    "data_temp['HL_StopWords_2'] = 'NaN'\n",
    "for i in range(data_temp.shape[0]):\n",
    "    data_temp.HL_StopWords_2.iloc[i] = no_user_alpha(data_temp['HL_PuncRem_1'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_temp.HL_StopWords_2.iloc[0])\n",
    "print(data_temp.headline.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Text -- NLTK’s built-in WordNetLemmatizer does this \n",
    "def normalization(headline_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_headline = []\n",
    "        for word in headline_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_headline.append(normalized_text)\n",
    "        return normalized_headline\n",
    "    \n",
    "data_temp['HL_Normalize_3'] = 'NaN'\n",
    "for i in range(data_temp.shape[0]):\n",
    "    data_temp.HL_Normalize_3.iloc[i] = normalization(data_temp['HL_StopWords_2'].iloc[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_temp.HL_Normalize_3.iloc[0])\n",
    "print(data_temp.headline.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "t = np.linspace(0, 10, 500)\n",
    "x = np.cos(2 * np.pi * t)\n",
    "y = np.sin(2 * np.pi * t)\n",
    "\n",
    "\n",
    "ax1.set_ylabel(u'cos(2\\u03c0t)')\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(-1, 1)\n",
    "plt.setp(ax1.get_xticklabels(),visible=False)\n",
    "\n",
    "ax2.set_xlabel('t')\n",
    "ax2.set_ylabel(u'sin(2\\u03c0t)')\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "lines = []\n",
    "for i in range(len(t)):\n",
    "    head = i - 1\n",
    "    head_slice = (t > t[i] - 1.0) & (t < t[i])\n",
    "    line1,  = ax1.plot(t[:i], x[:i], color='black')\n",
    "    line1a, = ax1.plot(t[head_slice], x[head_slice], color='red', linewidth=2)\n",
    "    line1e, = ax1.plot(t[head], x[head], color='red', marker='o', markeredgecolor='r')\n",
    "    line2,  = ax2.plot(t[:i], y[:i], color='black')\n",
    "    line2a, = ax2.plot(t[head_slice], y[head_slice], color='red', linewidth=2)\n",
    "    line2e, = ax2.plot(t[head], y[head], color='red', marker='o', markeredgecolor='r')\n",
    "    lines.append([line1,line1a,line1e,line2,line2a,line2e])\n",
    "\n",
    "\n",
    "# Build the animation using ArtistAnimation function\n",
    "\n",
    "ani = animation.ArtistAnimation(fig,lines,interval=50,blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
